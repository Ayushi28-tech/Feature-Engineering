{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8774209",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ef4bb0",
   "metadata": {},
   "source": [
    "1. What is a parameter?\n",
    "\n",
    " --> In machine learning, a parameter refers to a value that a model learns from the training data. These parameters are adjusted during training to improve the model's performance.\n",
    " ## Types of Parameters in Machine Learning\n",
    " #### Model Parameters (Learned from Data):\n",
    " These are values that the algorithm updates as it learns from the data.\n",
    " \n",
    " #### Hyperparameters (Manually Set):\n",
    " These are settings that are not learned from the data but are set before training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dd2ca8",
   "metadata": {},
   "source": [
    "2. What is correlation? What does negative correlation mean?\n",
    "\n",
    " --> Correlation is a statistical measure that describes the relationship between two variables—how they move in relation to each other. It helps determine whether an increase in one variable is associated with an increase or decrease in another.\n",
    "\n",
    " #### Positive Correlation: \n",
    " When one variable increases, the other also increases. (e.g., height and weight)\n",
    " #### Negative Correlation: \n",
    " When one variable increases, the other decreases. (e.g., hours spent studying and errors on a test)\n",
    " #### No Correlation: \n",
    " No relationship between the variables (e.g., shoe size and intelligence).\n",
    " ### What Does Negative Correlation Mean?\n",
    " A negative correlation means that as one variable increases, the other decreases, and vice versa. The correlation coefficient     (r) ranges from -1 to 1:\n",
    " \n",
    " r = -1: Perfect negative correlation (one goes up, the other always goes down).\n",
    " \n",
    " r between -1 and 0: Weak to strong negative correlation.\n",
    " \n",
    " r = 0: No correlation.\n",
    " ### Examples of Negative Correlation:\n",
    " The more time you exercise, the lower your body fat percentage.\n",
    " \n",
    " The more time spent on social media, the lower academic performance (in some cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344347a2",
   "metadata": {},
   "source": [
    "3. Define Machine Learning. What are the main components in Machine Learning?\n",
    "\n",
    " --> Machine Learning is a branch of artificial intelligence (AI) that enables computers to learn from data and make predictions or decisions without being explicitly programmed. Instead of following strict rules, ML models identify patterns in data and improve their performance over time.\n",
    " ### Main Components of Machine Learning\n",
    " #### Data\n",
    " The foundation of any ML system. High-quality, relevant data is crucial for training models.\n",
    " \n",
    " Types: Structured (tables, databases) vs. Unstructured (images, text, videos).\n",
    " #### Features (Input Variables)\n",
    " Measurable properties or characteristics of the data used for prediction.\n",
    " \n",
    " Feature Engineering involves selecting, transforming, or creating new features to improve model performance.\n",
    " #### Model\n",
    " The mathematical algorithm that learns patterns from data and makes predictions.\n",
    " \n",
    " Types:\n",
    " \n",
    "          Supervised (e.g., Linear Regression, Decision Trees)\n",
    "      \n",
    "          Unsupervised (e.g., K-Means, PCA)\n",
    "      \n",
    "          Reinforcement Learning (e.g., Q-learning, Deep Q-Networks)\n",
    " #### Training Process\n",
    " The phase where the model learns from labeled data by adjusting parameters (like weights in neural networks).\n",
    " \n",
    " Key Techniques: Gradient Descent, Backpropagation.\n",
    " #### Loss Function (Error Function)\n",
    " Measures how far the model’s predictions are from actual values. The goal is to minimize this error.\n",
    " \n",
    " Examples: Mean Squared Error (MSE), Cross-Entropy Loss.\n",
    " #### Optimization Algorithm\n",
    " Adjusts model parameters to minimize loss and improve performance.\n",
    " \n",
    " Examples: Stochastic Gradient Descent (SGD), Adam Optimizer.\n",
    " #### Hyperparameters\n",
    " Settings that are configured before training (not learned from data).\n",
    " \n",
    " Examples: Learning rate, number of hidden layers, batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007d18e",
   "metadata": {},
   "source": [
    "4. How does loss value help in determining whether the model is good or not?\n",
    "\n",
    " --> The loss value is a crucial metric in machine learning because it measures how well (or poorly) a model is performing. It quantifies the difference between the model's predictions and the actual values (ground truth). The goal of training is to minimize the loss so that the model makes better predictions.\n",
    " ### How to Determine If the Model Is Good?\n",
    " #### Compare Loss on Training and Validation Data\n",
    "\n",
    " Low Training & Low Validation Loss → Good model!\n",
    " \n",
    " Low Training & High Validation Loss → Overfitting (needs regularization or more diverse data).\n",
    " \n",
    " High Training & High Validation Loss → Underfitting (needs a more complex model or more training).\n",
    " #### Monitor Loss Over Training Epochs\n",
    " If loss decreases steadily, the model is learning well.\n",
    " \n",
    " If loss stagnates or fluctuates, optimization may need tuning (e.g., adjusting learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b314a7",
   "metadata": {},
   "source": [
    "5. What are continuous and categorical variables?\n",
    "\n",
    " --> In data science and machine learning, variables are classified into different types based on the kind of values they hold. Two major types are continuous and categorical variables.\n",
    " ### Continuous Variables\n",
    " A continuous variable can take an infinite number of values within a given range. These values are typically numerical and can   be measured with decimal precision.\n",
    " \n",
    " 🔹 Characteristics:\n",
    " \n",
    " Can be measured (not just counted).\n",
    " \n",
    " Can take fractional/decimal values.\n",
    " \n",
    " Often represented as real numbers.\n",
    " \n",
    " 🔹 Examples:\n",
    " \n",
    " Height of a person (e.g., 5.7 feet).\n",
    " \n",
    " Temperature (e.g., 36.5°C, 98.6°F).\n",
    " \n",
    " Time taken to complete a task (e.g., 12.3 seconds).\n",
    " \n",
    " Weight (e.g., 68.2 kg)\n",
    " ### Categorical Variables\n",
    " A categorical variable represents distinct groups or categories. These values are qualitative and do not have a natural           numerical meaning.\n",
    "\n",
    "  🔹 Characteristics:\n",
    "\n",
    "  Can be counted but not measured.\n",
    " \n",
    "  Values belong to fixed categories.\n",
    " \n",
    "  Can be nominal (unordered) or ordinal (ordered).\n",
    " \n",
    " 🔹 Types:\n",
    " \n",
    " ✅ Nominal Variables (No order)\n",
    "\n",
    "   Example: Colors (Red, Blue, Green), Gender (Male, Female, Non-binary).\n",
    "   \n",
    " ✅ Ordinal Variables (Ordered categories)\n",
    " \n",
    "   Example: Education Level (High School < Bachelor's < Master's < Ph.D.).\n",
    "   \n",
    "  🔹 Examples:\n",
    "\n",
    " Marital Status (Single, Married, Divorced).\n",
    " \n",
    " Blood Type (A, B, AB, O).\n",
    " \n",
    " Movie Genre (Action, Comedy, Drama)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6397ff",
   "metadata": {},
   "source": [
    "6. How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "\n",
    " --> Categorical variables must be converted into numerical form before they can be used in machine learning models. Below are the common techniques for handling categorical variables.\n",
    " ### Encoding Techniques\n",
    " ✅ 1.1 One-Hot Encoding (OHE)\n",
    " \n",
    "   Converts each category into a binary (0 or 1) column.\n",
    " \n",
    "   Works well for nominal (unordered) categories.\n",
    " \n",
    "   Increases feature dimensions (curse of dimensionality for high-cardinality data).\n",
    "  \n",
    " ✅ 1.2 Label Encoding\n",
    " \n",
    " Assigns a unique integer to each category.\n",
    " \n",
    " Works for ordinal categories (e.g., \"Low\" < \"Medium\" < \"High\").\n",
    " \n",
    " Not ideal for nominal categories since numbers may mislead ML models.\n",
    " \n",
    " ✅ 1.3 Ordinal Encoding\n",
    " \n",
    " Similar to Label Encoding but manually assigns an order.\n",
    "\n",
    " Useful when categories have a clear ranking (e.g., Education Level).\n",
    " \n",
    " ✅ 1.4 Frequency Encoding\n",
    " \n",
    " Assigns each category the frequency (count) of its occurrence.\n",
    " \n",
    " Can be useful for handling high-cardinality categorical data.\n",
    " \n",
    " ✅ 1.5 Target Encoding (Mean Encoding)\n",
    " \n",
    " Replaces categories with the mean of the target variable for that category.\n",
    " \n",
    " Works well in high-cardinality categorical data, but prone to data leakage.\n",
    " ### Handling High-Cardinality Categorical Data\n",
    " If a categorical variable has too many unique values, encoding it can lead to issues. Here are some strategies:\n",
    "\n",
    " Feature Hashing: Hashes categorical values into a fixed number of buckets.\n",
    " \n",
    " Grouping Rare Categories: Merge less frequent categories into an “Other” category.\n",
    " \n",
    " Dimensionality Reduction (e.g., PCA on encoded features)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440ef92",
   "metadata": {},
   "source": [
    "7. What do you mean by training and testing a dataset?\n",
    "\n",
    " --> In machine learning, a dataset is typically split into two (or three) parts: training, testing, and sometimes validation. This is done to ensure that the model learns from past data and generalizes well to new, unseen data.\n",
    " ### Training Dataset\n",
    " The training set is used to train (teach) the machine learning model.\n",
    " \n",
    " The model learns patterns from this data by adjusting its internal parameters (weights, biases).\n",
    " \n",
    " Typically makes up 70–80% of the total dataset.\n",
    " ### Testing Dataset\n",
    " The testing set is used to evaluate the model's performance on unseen data.\n",
    " \n",
    " It helps check how well the model generalizes to new inputs.\n",
    " \n",
    " Typically 20–30% of the total dataset.\n",
    " \n",
    " The model does not learn from this data—it is used only for final assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2eb882",
   "metadata": {},
   "source": [
    "8. What is sklearn.preprocessing?\n",
    "\n",
    " --> sklearn.preprocessing is a module in Scikit-Learn that provides functions for transforming raw data into a format suitable for machine learning models. Many ML algorithms require numerical, scaled, and normalized inputs, and this module helps prepare the data accordingly.\n",
    " ### Why Use sklearn.preprocessing?\n",
    " ✅ Ensures better model performance by normalizing inputs.\n",
    "\n",
    " ✅ Helps handle categorical variables efficiently.\n",
    "\n",
    " ✅ Reduces the impact of outliers.\n",
    "\n",
    " ✅ Makes training faster and more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3fbf5",
   "metadata": {},
   "source": [
    "9. What is a Test set?\n",
    "\n",
    " --> A test set is a portion of the dataset that is kept separate from the training data and is used to evaluate the performance of a trained machine learning model. The test set simulates real-world data and helps determine how well the model generalizes to unseen data.\n",
    " ### Why Is the Test Set Important?\n",
    " ✅ Measures Generalization → Checks if the model performs well on new, unseen data.\n",
    "\n",
    " ✅ Prevents Data Leakage → Ensures that the model does not memorize patterns from the training data.\n",
    "\n",
    " ✅ Helps Compare Models → Used to fairly compare different ML algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ebd2b9",
   "metadata": {},
   "source": [
    "10. How do we split data for model fitting (training and testing) in Python? How do you approach a Machine Learning problem?\n",
    "\n",
    " --> We typically use Scikit-Learn's train_test_split to divide the dataset into training and testing sets.\n",
    " \n",
    " 🔹 Example: Splitting Data (80% Train, 20% Test)\n",
    " \n",
    "  from sklearn.model_selection import train_test_split\n",
    "  \n",
    "  import pandas as pd\n",
    "\n",
    "  #Sample dataset\n",
    "  \n",
    "  data = pd.DataFrame({'Feature1': [1, 2, 3, 4, 5],\n",
    "                     'Feature2': [10, 20, 30, 40, 50],\n",
    "                     'Target': [0, 1, 0, 1, 0]})\n",
    "\n",
    "  #Splitting data (80% train, 20% test)\n",
    "  \n",
    "  train_set, test_set = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "  print(\"Training Set:\\n\", train_set)\n",
    "  \n",
    "  print(\"Test Set:\\n\", test_set)\n",
    "  \n",
    "  ### How to Approach a Machine Learning Problem?\n",
    " A structured approach to solving an ML problem involves several key steps.\n",
    "\n",
    "  Step 1: Define the Problem\n",
    "  \n",
    "  🔹 What is the goal? (e.g., Predict house prices, detect fraud)\n",
    "  \n",
    "  🔹 Is it a classification, regression, or clustering problem?\n",
    "  \n",
    "  Step 2: Data Collection & Understanding\n",
    "  \n",
    "  🔹 Collect the dataset (CSV, database, API, etc.).\n",
    "  \n",
    "  🔹 Explore the data with pandas & matplotlib.\n",
    "  \n",
    "  Step 3: Data Preprocessing\n",
    "  \n",
    "  🔹 Handle missing values (SimpleImputer)\n",
    "  \n",
    "  🔹 Encode categorical variables (OneHotEncoder, LabelEncoder)\n",
    "  \n",
    "  🔹 Scale numerical features (StandardScaler, MinMaxScaler)\n",
    "  \n",
    "  Step 4: Splitting Data (Train-Test Split)\n",
    "  \n",
    "  🔹 80% Train – 20% Test (For standard ML tasks).\n",
    "  \n",
    "  🔹 70% Train – 15% Validation – 15% Test (For deep learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d0586b",
   "metadata": {},
   "source": [
    "11. Why do we have to perform EDA before fitting a model to the data?\n",
    "\n",
    " --> Exploratory Data Analysis (EDA) is a critical step before training a machine learning model because it helps us understand the dataset, identify potential issues, and make informed decisions about preprocessing and model selection.\n",
    " ### Key Reasons for Performing EDA\n",
    " 1️⃣ Understanding the Data Structure\n",
    " \n",
    "   Before fitting a model, we need to know:\n",
    "   \n",
    "           ✅ Number of rows & columns\n",
    "    \n",
    "           ✅ Data types (numerical, categorical, text, etc.)\n",
    "    \n",
    "           ✅ Presence of missing values\n",
    "       \n",
    " 2️⃣ Detecting Missing Values\n",
    "  \n",
    "  Missing data can lead to biased or inaccurate models. EDA helps determine:\n",
    "  \n",
    "           ✅ Which features have missing values?\n",
    "       \n",
    "           ✅ How should we handle them? (e.g., imputation or removal)\n",
    "           \n",
    " 3️⃣ Detecting Outliers\n",
    "  \n",
    "   Outliers can distort a model’s predictions. EDA helps identify them using:\n",
    "\n",
    "           ✅ Box plots (Visualizing outliers)\n",
    "           \n",
    "           ✅ Z-scores or IQR (Interquartile Range)\n",
    "   \n",
    " 4️⃣ Understanding Feature Distributions\n",
    "   \n",
    "   EDA helps determine whether features follow a normal distribution or are skewed, which influences preprocessing steps.\n",
    "   \n",
    "           ✅ If normally distributed → Use StandardScaler\n",
    "           \n",
    "           ✅ If skewed → Use log transformation\n",
    "   \n",
    " 5️⃣ Detecting Correlations Between Features\n",
    "  \n",
    "   EDA helps us identify:\n",
    "   \n",
    "           ✅ Highly correlated features (Can cause multicollinearity in regression models)\n",
    "           \n",
    "           ✅ Weakly correlated features (May not contribute much to predictions)\n",
    "   \n",
    " 6️⃣ Identifying Categorical Variables & Encoding Needs\n",
    "  \n",
    "   EDA helps determine:\n",
    "   \n",
    "           ✅ How many unique categories exist in each categorical feature?\n",
    "           \n",
    "           ✅ Which encoding method is best? (One-Hot Encoding vs. Label Encoding)\n",
    "  \n",
    "  7️⃣ Feature Selection & Engineering\n",
    "  \n",
    "  EDA helps determine:\n",
    "  \n",
    "           ✅ Which features are important? (Using feature importance or correlation)\n",
    "           \n",
    "           ✅ Do we need to create new features? (Feature engineering)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26e53a6d",
   "metadata": {},
   "source": [
    "12 and 13 same as 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138e21d2",
   "metadata": {},
   "source": [
    "14. How can you find correlation between variables in Python?\n",
    "\n",
    " --> Correlation measures the relationship between two variables. It helps determine how strongly changes in one variable correspond to changes in another.\n",
    " \n",
    " 1️⃣ Using Pandas corr() for Correlation Matrix\n",
    " \n",
    " Pandas provides the .corr() function to calculate correlations between numerical variables in a dataset.\n",
    " \n",
    " 2️⃣ Visualizing Correlation Matrix Using Heatmap\n",
    " \n",
    " A heatmap makes it easier to spot strong and weak correlations.\n",
    " \n",
    " 3️⃣ Choosing the Right Correlation Method\n",
    " There are three main correlation methods in Pandas: pearson(default), spearman, kendall\n",
    " \n",
    " 4️⃣ Finding Correlation Between Specific Columns\n",
    " \n",
    " If you only need correlation between two variables:\n",
    "             \n",
    "             correlation = df[\"Age\"].corr(df[\"Salary\"])\n",
    "             print(\"Correlation between Age and Salary:\", correlation)\n",
    "  5️⃣ Handling Categorical Variables in Correlation\n",
    "  \n",
    "  Categorical variables need to be encoded before correlation analysis.\n",
    "  \n",
    "   ✅ Use Label Encoding for ordinal categories.\n",
    "   \n",
    "   ✅ Use One-Hot Encoding for non-ordinal categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b051d",
   "metadata": {},
   "source": [
    "15. What is causation? Explain difference between correlation and causation with an example.\n",
    "\n",
    " --> Causation (also called cause-and-effect) means that one event directly influences another. If A causes B, then a change in A produces a change in B.\n",
    "\n",
    " ✅ Example of Causation:\n",
    "\n",
    "  More study hours cause higher exam scores.\n",
    "\n",
    "  Increasing temperature causes ice cream sales to rise.\n",
    "  \n",
    "  ### Difference Between Correlation and Causation\n",
    "  #### Correlation:\n",
    "  ✅ Definition: A statistical relationship between two variables.\n",
    "  \n",
    "  ✅ Direction: Can be positive or negative.\n",
    "  \n",
    "  ✅ Third factors: Can be influenced by an external factor.\n",
    "  \n",
    "  ✅ Proof: Statistical tests can show correlation.\n",
    "  \n",
    "  #### Causation:\n",
    "  ✅ Definition: One variable directly affects another.\n",
    "  \n",
    "  ✅ Direction: Always has a cause-effect direction.\n",
    "  \n",
    "  ✅ Third factors: Not affected by external variables.\n",
    "  \n",
    "  ✅ Proof: Requires experiments or controlled studies.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7853aef3",
   "metadata": {},
   "source": [
    "16. What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "\n",
    " --> An optimizer in Machine Learning and Deep Learning is an algorithm that adjusts model parameters (weights and biases) to minimize the loss function during training. It helps the model learn faster and more efficiently by updating weights based on the gradients calculated during backpropagation.\n",
    "\n",
    "  ✅ Goal: Reduce the error (loss) and improve model performance.\n",
    "  \n",
    "  ✅ How? By adjusting weights in the direction that minimizes loss.\n",
    "  \n",
    "  ### Types of Optimizers\n",
    "  🔹 1️⃣ Gradient Descent (GD)\n",
    "  \n",
    "    Gradient Descent is the most fundamental optimizer. It updates weights based on the gradient of the loss function.\n",
    "  \n",
    "  🔹 2️⃣ Momentum Optimizer\n",
    "  \n",
    "    Fixes the problem of oscillations in SGD.\n",
    "  \n",
    "    Uses a moving average of past gradients to smooth updates.\n",
    "  \n",
    "  🔹 3️⃣ AdaGrad (Adaptive Gradient Algorithm)\n",
    "  \n",
    "   Adjusts learning rates individually for each parameter.\n",
    "   \n",
    "   Good for sparse data, but learning rate decays over time.\n",
    "   \n",
    "  🔹 4️⃣ RMSprop (Root Mean Square Propagation)\n",
    "  \n",
    "   Fixes AdaGrad’s problem of decreasing learning rates.\n",
    "   \n",
    "   Uses a moving average of squared gradients to adapt learning rates.\n",
    "   \n",
    "   Common in RNNs and NLP tasks.\n",
    "   \n",
    "  🔹 5️⃣ Adam (Adaptive Moment Estimation) – Most Popular\n",
    "  \n",
    "   Combines Momentum & RMSprop for the best performance.\n",
    "    \n",
    "   Uses two moving averages:\n",
    "   \n",
    "   𝑚𝑡→ Mean of gradients (Momentum).\n",
    "   \n",
    "   𝑣𝑡→ Mean of squared gradients (RMSprop).\n",
    "   \n",
    "  🔹 6️⃣ AdamW (Adam with Weight Decay)\n",
    "  \n",
    "   Fixes Adam's overfitting issue by adding weight decay.\n",
    "   \n",
    "   Prevents weights from growing too large, improving generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2739fa",
   "metadata": {},
   "source": [
    "17. What is sklearn.linear_model ?\n",
    "\n",
    " --> sklearn.linear_model is a module in Scikit-Learn that provides various linear models for regression and classification tasks. These models assume a linear relationship between input features and the target variable.\n",
    " ### Types of Models in sklearn.linear_model\n",
    " 1️⃣ Linear Regression (For Regression)\n",
    " \n",
    " 2️⃣ Logistic Regression (For Classification)\n",
    " \n",
    " 3️⃣ Ridge Regression (L2 Regularization)\n",
    " \n",
    " 4️⃣ Lasso Regression (L1 Regularization)\n",
    " \n",
    " 5️⃣ ElasticNet (Combination of L1 & L2 Regularization)\n",
    " \n",
    " 6️⃣ Perceptron (For Binary Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cef064",
   "metadata": {},
   "source": [
    "18. What does model.fit() do? What arguments must be given?\n",
    "\n",
    " --> The .fit() method is used in Scikit-Learn (and other ML libraries) to train a machine learning model on a dataset. It finds the optimal parameters (weights and biases) that minimize the loss function.\n",
    "\n",
    " ✅ What Happens Internally?\n",
    "\n",
    " Reads the training data (features X and target y).\n",
    " \n",
    " Computes gradients (if applicable).\n",
    " \n",
    " Optimizes the model parameters using the selected algorithm.\n",
    " \n",
    " Stores the learned parameters for future predictions (model.predict()).\n",
    " ### Arguments for model.fit()\n",
    " The required arguments depend on the model type.\n",
    "\n",
    " 1️⃣ Supervised Learning Models (LinearRegression, LogisticRegression, etc.)\n",
    " \n",
    " 2️⃣ Models with Extra Parameters (e.g., Neural Networks, SVMs)\n",
    " \n",
    "  Some models accept additional arguments, such as batch size, epochs, or sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37135268",
   "metadata": {},
   "source": [
    "19. What does model.predict() do? What arguments must be given?\n",
    "\n",
    " --> The .predict() method is used in Scikit-Learn (and other ML libraries) to make predictions based on a trained model. Once a model is trained using model.fit(), we can use model.predict(X_new) to get predictions on new, unseen data.\n",
    "\n",
    " ✅ What Happens Internally?\n",
    "\n",
    "  Takes input features (X_new).\n",
    " \n",
    "  Applies the trained model's parameters (weights & biases).\n",
    " \n",
    "  Computes and returns predicted outputs (regression values or classification labels).\n",
    "  ### Arguments for model.predict()\n",
    "          model.predict(X_new)\n",
    "  \n",
    "  X_new → New feature matrix (same format as training X).\n",
    "  \n",
    "  Returns: An array of predictions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "16da2af9",
   "metadata": {},
   "source": [
    "20 same as 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebd7837",
   "metadata": {},
   "source": [
    "21. What is feature scaling? How does it help in Machine Learning?\n",
    "\n",
    " --> Feature scaling is the process of normalizing or standardizing the range of independent variables (features) in a dataset. It ensures that all features contribute equally to the model by bringing them to a similar scale.\n",
    "\n",
    "🔹 Why is it important?\n",
    "\n",
    "  Prevents dominance of large values: Features with larger magnitudes (e.g., income in thousands vs. age in years) can dominate    smaller ones.\n",
    "  \n",
    "  Speeds up optimization: Gradient descent converges faster when features are scaled.\n",
    "  \n",
    "  Essential for distance-based models: Algorithms like KNN, K-Means, PCA, SVM rely on distance calculations, which are affected    by unscaled features.\n",
    "  \n",
    "  Improves model performance: Helps avoid bias in ML models that assume normally distributed data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e4504",
   "metadata": {},
   "source": [
    "22. How do we perform scaling in Python?\n",
    "\n",
    " --> Feature scaling in Python can be done using Scikit-Learn's preprocessing module. The two most common techniques are Standardization (Z-score normalization) and Min-Max Scaling (Normalization).\n",
    " \n",
    " 1️⃣ Standardization (Z-score Normalization)\n",
    "   \n",
    "   🔹 Used when data follows a normal distribution (mean ≈ 0, std ≈ 1).\n",
    "   \n",
    "   🔹 Best for algorithms like Logistic Regression, SVM, and Neural Networks.\n",
    "   \n",
    " 2️⃣ Min-Max Scaling (Normalization)\n",
    "    \n",
    "    Scales values between a fixed range (default: 0 to 1).\n",
    "    \n",
    "    Used when data is not normally distributed or has a fixed range (e.g., pixel values 0-255).\n",
    "    \n",
    "    Best for KNN, K-Means, Neural Networks.\n",
    "    \n",
    " 3️⃣ Robust Scaling (For Outliers)\n",
    "    \n",
    "    IQR (Interquartile Range) = Q3 - Q1\n",
    "    \n",
    "    Resistant to outliers (Unlike Standardization & Min-Max).\n",
    "    \n",
    "    Best for datasets with extreme values."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c70dbb35",
   "metadata": {},
   "source": [
    "23 and 24 already done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf653b5",
   "metadata": {},
   "source": [
    "25. Explain data encoding?\n",
    "\n",
    " --> Data encoding is the process of converting categorical (non-numeric) data into numerical form so that machine learning models can understand and process it. Most ML algorithms work with numbers, so categorical data must be transformed before training.\n",
    " ### Types of Data Encoding\n",
    " 1️⃣ Label Encoding\n",
    "  Assigns a unique integer to each category.\n",
    "  \n",
    "  Useful for ordinal data (where categories have a natural order).\n",
    "  \n",
    "  Limitation: The model may misinterpret the numerical values as meaningful differences.\n",
    "  \n",
    " 2️⃣ One-Hot Encoding\n",
    " \n",
    "  Converts each category into a binary vector.\n",
    "  \n",
    "  Avoids numerical misinterpretation (solves Label Encoding’s issue).\n",
    "  \n",
    "  Best for nominal data (categories without order).\n",
    "  \n",
    " 3️⃣ Ordinal Encoding\n",
    " \n",
    "  Similar to Label Encoding but preserves order in categorical data.\n",
    "  \n",
    "  Best for ordinal categories (e.g., Low < Medium < High).\n",
    "  \n",
    " 4️⃣ Target (Mean) Encoding\n",
    " \n",
    "  Replaces categories with the mean of the target variable for each category.\n",
    "  \n",
    "  Best for high-cardinality categorical variables (many unique values).\n",
    "  \n",
    "  Risk: May cause data leakage if not done properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30a5cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
